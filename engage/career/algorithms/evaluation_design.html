<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />

    <title>HermodLabs — Home Page</title>
    <meta
      name="description"
      content="HermodLabs builds a co-timing engine that proves streams are on the same clock before you subtract, compare, threshold, or interpret. Portable cancellation + diagnostics + proof-shaped certificates."
    />

    <link rel="stylesheet" href="/css/main.css" />
  </head>

<body>
<site-page>
  <section class="job" aria-labelledby="job-title">
    <div class="job__inner">
      <header class="job__header">
        <p class="job__eyebrow">Careers</p>
        <h1 class="job__title" id="job-title">Applied Statistician / Experimentation Lead (Evaluation Design)</h1>
        <p class="job__subhead">
          Focus: backtesting, uncertainty calibration, alert-quality measurement, pilot evaluation protocols, and “proof with receipts.”
        </p>
      </header>

      <section class="job__section" aria-labelledby="about-role-title">
        <h2 class="job__heading" id="about-role-title">About the Role</h2>
        <p class="job__text">
          Our product lives in the gap between interesting plots and decisions you can bet operations on. We already have
          deterministic machinery (co-timing, validity gating, nuisance cancellation). None of it matters if we can’t
          evaluate performance honestly—without leakage, without cherry-picked windows, and with metrics that match real
          operator outcomes.
        </p>
        <p class="job__text">
          This role owns the evaluation truth layer: statistical rigor, experimentation design, and measurement protocols
          that produce defensible claims with receipts. You make it possible to say “this alert is reliable” or “this
          model gives lead time” and back it up.
        </p>
      </section>

      <section class="job__section" aria-labelledby="own-title">
        <h2 class="job__heading" id="own-title">What You’ll Own</h2>
        <ul class="job__list">
          <li class="job__list-item">
            <strong>Evaluation methodology:</strong> temporal splits, leakage prevention, backtesting protocols, and event-based scoring.
          </li>
          <li class="job__list-item">
            <strong>Metric design for operations:</strong> early-warning lead time, alarm precision/recall, alert-fatigue metrics,
            validity uptime, stability/repeatability.
          </li>
          <li class="job__list-item">
            <strong>Uncertainty and calibration:</strong> confidence intervals, reliability curves, calibration diagnostics, abstention semantics.
          </li>
          <li class="job__list-item">
            <strong>Experimentation frameworks for pilots:</strong> before/after interventions, staged rollouts, controlled trials, minimal-run validation plans.
          </li>
          <li class="job__list-item">
            <strong>Evidence packaging:</strong> evaluation receipts that tie results to datasets, gates, versions, and assumptions.
          </li>
        </ul>
      </section>

      <section class="job__section" aria-labelledby="do-title">
        <h2 class="job__heading" id="do-title">What You’ll Do</h2>
        <ul class="job__list">
          <li class="job__list-item">
            Define success metrics per pilot type (cigar lounge, data center, noise “air bubble”) with clear operational meaning.
          </li>
          <li class="job__list-item">
            Build backtesting systems: rolling-origin evaluation for forecasting, event-based evaluation for anomaly/alerts, stratified analysis by regime/site.
          </li>
          <li class="job__list-item">
            Design trust-aware evaluation: score performance conditioned on validity gates and measure “valid measurement uptime” as a first-class output.
          </li>
          <li class="job__list-item">
            Quantify uncertainty: prediction intervals, effect sizes for interventions, calibration checks for probabilistic outputs.
          </li>
          <li class="job__list-item">
            Develop pilot protocols: what to collect, how long, what constitutes a comparable day, and how to document confounders.
          </li>
          <li class="job__list-item">
            Work cross-functionally with DSP/ML/product/field teams so evaluation matches reality and outputs are decision-ready.
          </li>
        </ul>
      </section>

      <section class="job__section" aria-labelledby="deliverables-title">
        <h2 class="job__heading" id="deliverables-title">Concrete Deliverables</h2>
        <ul class="job__list">
          <li class="job__list-item">
            A Pilot Evaluation Playbook: standardized scoring protocols and metrics for each pilot type.
          </li>
          <li class="job__list-item">
            A backtesting harness with time-series split logic, event labeling/scoring, and automated reporting (plots + tables + narrative summary).
          </li>
          <li class="job__list-item">
            A calibration &amp; uncertainty toolkit: reliability curves, coverage checks for prediction intervals, abstention reporting standards.
          </li>
          <li class="job__list-item">
            An evidence receipt format: dataset hashes/versions, validity gate definitions, model/config versions, evaluation code version.
          </li>
          <li class="job__list-item">
            A decision-ready readout template that turns results into deploy / iterate / no-go, with quantified uncertainty.
          </li>
        </ul>
      </section>

      <section class="job__section" aria-labelledby="req-title">
        <h2 class="job__heading" id="req-title">Required Qualifications</h2>
        <ul class="job__list">
          <li class="job__list-item">
            Strong applied statistics background with real-world evaluation experience (time series, anomaly detection, operational metrics).
          </li>
          <li class="job__list-item">
            Experience designing experiments where ground truth is imperfect and regimes shift (nonstationarity).
          </li>
          <li class="job__list-item">
            Comfort with uncertainty quantification and calibration for probabilistic systems.
          </li>
          <li class="job__list-item">
            Ability to implement evaluation pipelines in Python/R and produce clear, reproducible reports.
          </li>
        </ul>
      </section>

      <section class="job__section" aria-labelledby="pref-title">
        <h2 class="job__heading" id="pref-title">Preferred Qualifications</h2>
        <ul class="job__list">
          <li class="job__list-item">
            Experience evaluating monitoring/alerting systems (precision/recall under class imbalance, alert fatigue, incident-based scoring).
          </li>
          <li class="job__list-item">
            Familiarity with causal inference / quasi-experimental design (before/after, difference-in-differences) for pilot interventions.
          </li>
          <li class="job__list-item">
            Experience working with sensor/telemetry data and messy operational logs.
          </li>
          <li class="job__list-item">
            Comfort collaborating with ML and DSP teams on metric definitions and failure-mode interpretation.
          </li>
        </ul>
      </section>

      <section class="job__section" aria-labelledby="success-title">
        <h2 class="job__heading" id="success-title">How You’ll Be Measured (First 60–90 Days)</h2>
        <ul class="job__list">
          <li class="job__list-item">
            You ship a v1 evaluation playbook + backtesting harness used in at least one pilot readout.
          </li>
          <li class="job__list-item">
            The team stops arguing with vibes because splits, metrics, and reporting are explicit and reproducible.
          </li>
          <li class="job__list-item">
            You identify at least one evaluation pitfall (leakage, confounding, mis-scoring abstention) and fix it before it becomes a public claim.
          </li>
          <li class="job__list-item">
            Pilot outcomes become decision-ready: clear go/no-go gates, uncertainty bounds, and crisp next experiments.
          </li>
        </ul>
      </section>

      <section class="job__section" aria-labelledby="style-title">
        <h2 class="job__heading" id="style-title">Working Style</h2>
        <ul class="job__list">
          <li class="job__list-item">You’re allergic to accidental leakage and “test on the training week.”</li>
          <li class="job__list-item">You prefer effect sizes and uncertainty bounds over point estimates.</li>
          <li class="job__list-item">You make evaluation understandable: stakeholders can see what was measured, why it matters, and what the risks are.</li>
        </ul>
      </section>

      <section class="job__section" aria-labelledby="level-title">
        <h2 class="job__heading" id="level-title">Title &amp; Level</h2>
        <p class="job__text">
          Applied Statistician / Experimentation Lead (Evaluation Design) (senior IC; scope can scale), partnering with
          DSP/estimation, Scientific ML, validation, product/field, and data engineering.
        </p>
      </section>

      <section class="job__section job__section--apply" aria-labelledby="apply-title">
        <h2 class="job__heading" id="apply-title">Apply</h2>
        <p class="job__text">Send a short note and your resume.</p>

        <form
          class="job__form"
          action="/careers/apply/applied-statistician-experimentation"
          method="post"
          enctype="multipart/form-data"
        >
          <div class="job__field">
            <label class="job__label" for="app-name">Name</label>
            <input class="job__input" id="app-name" name="name" type="text" autocomplete="name" required />
          </div>

          <div class="job__field">
            <label class="job__label" for="app-email">Email</label>
            <input class="job__input" id="app-email" name="email" type="email" autocomplete="email" required />
          </div>

          <div class="job__field">
            <label class="job__label" for="app-message">Message</label>
            <textarea
              class="job__textarea"
              id="app-message"
              name="message"
              rows="7"
              placeholder="A few sentences about your background and why this role fits."
              required
            ></textarea>
          </div>

          <div class="job__field">
            <label class="job__label" for="app-resume">Resume</label>
            <input class="job__input" id="app-resume" name="resume" type="file" accept=".pdf,.doc,.docx" required />
          </div>

          <div class="job__actions">
            <button class="button button--primary" type="submit">Submit</button>
            <a class="button button--secondary" href="/careers">Back to roles</a>
          </div>

          <p class="job__fineprint">We only use this to respond to your application. No spam.</p>
        </form>
      </section>
    </div>
  </section>
</site-page>


  <script type="module" src="/components/site_page/site_page.js"></script>
  <script type="module" src="/components/site_header/site_header.js"></script>
  <script type="module" src="/components/site_footer/site_footer.js"></script>

  <script type="module" src="/components/site_hero/site_hero.js"></script>
  <script type="module" src="/components/social_proof/social_proof.js"></script>
  <script type="module" src="/components/site_about/site_about.js"></script>
  <script type="module" src="/components/event_invite/event_invite.js"></script>
</body>
</html>
