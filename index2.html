<html>
  <head>
    <title>HermodLabs — Co-Timing Engine (Public Whitepaper)</title>
    <meta
      name="description"
      content="HermodLabs builds a co-timing engine that proves streams are on the same clock before you subtract, compare, threshold, or interpret. Portable cancellation + diagnostics + proof-shaped certificates."
    />
    <link rel="stylesheet" href="/css/main.css" />
    <link rel="stylesheet" href="/css/scratch.css" />
    <link rel="stylesheet" href="/css/override.css" />
  </head>

  <body>
    <div class="page">
      <site-header></site-header>

      <main>
        <!-- =========================================================
             HERO — Lead-gen aligned with public-cut whitepaper
        ========================================================== -->
        <section class="hero-section" id="overview">
          <div class="hero-copy">
            <p class="hero-copy__label">
              Co-Timing Engine &bull; Portable cancellation &bull; Timing integrity for sensor-heavy environments
            </p>
            <p class="hero-copy__title">
              Stop subtracting signals that aren&rsquo;t on the same clock.
            </p>
            <p class="hero-copy__body">
When you have more than one sensor, it&rsquo;s easy to assume they&rsquo;re all keeping time the same way.
But small timing differences add up, and comparisons start to lie which produce leftovers that look important
but are just misalignment. HermodLabs lines things up first so comparisons behave, and gives you clear
signals about when the result is solid versus when it needs a different setup.

            </p>

            <p class="page-section__body" style="margin-top: 1rem;">
              <a class="page-section__link" href="WHITEPAPER_PDF_URL_HERE">
                Download the public-cut whitepaper &rarr;
              </a>
              <span style="display: inline-block; margin-left: 0.75rem;"></span>
              <a class="page-section__link" href="#next-step">
                Request a data evaluation &rarr;
              </a>
            </p>
          </div>

          <div class="hero-video">
            <div
              class="hero-video__frame"
              aria-label="HermodLabs whitepaper and demo overview (placeholder)"
            >
              <button
                class="hero-video__play"
                type="button"
                aria-label="Play overview video"
              >
                <span class="hero-video__play-icon"></span>
              </button>
              <div class="hero-video__meta">
                <span class="hero-video__meta-title">Watch the 2:00 overview</span>
                <span class="hero-video__meta-label">Video &middot; Coming soon</span>
              </div>
            </div>
          </div>
        </section>

<!-- =========================================================
     PROBLEM — The problem you actually have (data centers version)
========================================================== -->
<section id="the-problem" aria-label="The problem you actually have">
  <div class="core-point">
    <h2 class="core-point__action-title">
      When something looks &ldquo;wrong,&rdquo; you can&rsquo;t tell if the data hall changed&hellip; or your measurement timing did.
    </h2>

    <div class="supporting-detail">
      <h3 class="supporting-detail__title">
        <a class="article-link" href="article/timing_lies_false_residuals/index.html">
          False alarms you can&rsquo;t reproduce (the &ldquo;ghost delta&rdquo; problem)
        </a>
      </h3>
      <div class="supporting-detail__author">What ops sees &rarr; what&rsquo;s actually happening</div>
      <div class="supporting-detail__desc">
        <p class="supporting-detail__desc--paragraph">
          Picture two temperature sensors watching the same aisle. A real hot spot forms and both should move together.
          But in practice, one sensor reports a little late, another reports a little early, and a third is timestamped by a different box.
        </p>
        <p class="supporting-detail__desc--paragraph">
          Now you do the normal thing: compare sensors, subtract baselines, trigger an alert on the difference.
          You can accidentally create a scary-looking gap that is not a real thermal event &mdash; it is just the same change
          arriving at slightly different times.
        </p>
        <p class="supporting-detail__desc--paragraph">
          That is how you get tickets like: &ldquo;Aisle 12 is drifting&rdquo; on Tuesday, &ldquo;seems fine&rdquo; on Wednesday,
          and &ldquo;the model is moody&rdquo; by Friday. The system is reacting to timing mismatch, not physics.
        </p>
      </div>
      <div class="supporting-detail__image">
        <img
          height="300"
          width="400"
          src="img/01_phasor_misalignment_residual_chord.png"
          alt="Illustration: the same change arriving at different times can look like a real difference"
        />
      </div>
    </div>

    <div class="supporting-detail">
      <h3 class="supporting-detail__title">
        <a class="article-link" href="article/why_now_timing_integrity/index.html">
          Why this gets worse in modern data centers
        </a>
      </h3>
      <div class="supporting-detail__author">More sensors, more vendors, tighter control</div>
      <div class="supporting-detail__desc">
        <p class="supporting-detail__desc--paragraph">
          Years ago, you might have had one building system and a few steady sensors. Now you have dense sensor grids,
          mixed vendors, edge boxes, gateways, and dashboards pulling data at different rates.
          Everything looks &ldquo;timestamped,&rdquo; but those timestamps don&rsquo;t necessarily mean the same thing.
        </p>
        <p class="supporting-detail__desc--paragraph">
          Meanwhile the environment is intentionally dynamic: fans ramp, dampers move, containment leaks,
          CRAC behavior changes, doors open, load shifts. These real changes are fast &mdash; so even small timing offsets
          can make two sensors look like they disagree.
        </p>
        <p class="supporting-detail__desc--paragraph">
          And when you automate decisions, timing mistakes scale up: a tiny alignment problem can turn into unnecessary alarms,
          wasted tuning time, or control actions that chase a problem that isn&rsquo;t there.
        </p>
      </div>
      <div class="supporting-detail__image">
        <img
          height="300"
          width="400"
          src="img/02_distributed_sensors_mixed_clocks.png"
          alt="Illustration: distributed sensing and mixed systems increase timing mismatch risk"
        />
      </div>
    </div>

    <div class="supporting-detail">
      <h3 class="supporting-detail__title">
        <a class="article-link" href="article/buyer_checklist/index.html">
          Fast checklist for buyers (data-center practical)
        </a>
      </h3>
      <div class="supporting-detail__author">How to spot signal vs scheduling noise</div>
      <div class="supporting-detail__desc">
        <p class="supporting-detail__desc--paragraph">
          Ask this first: when you compare two sensors, how do you make sure you are comparing the <em>same moment</em> in time?
          (&ldquo;Same minute&rdquo; is not enough when fans and loads change quickly.)
        </p>
        <p class="supporting-detail__desc--paragraph">
          When the system flags a delta, does it tell you whether it was caused by a real change in the hall
          or by data arriving late, batching, resampling, gateway buffering, or mixed polling rates?
        </p>
        <p class="supporting-detail__desc--paragraph">
          If the process is basically &ldquo;tune thresholds until the dashboard feels calm,&rdquo; you are buying a demo.
          A reliable system can explain why a difference is believable and when it is not.
        </p>
      </div>
      <div class="supporting-detail__image">
        <img
          height="300"
          width="400"
          src="img/03_buyer_checklist_certificates.png"
          alt="Illustration: checklist for repeatable, trustworthy comparisons across sensors"
        />
      </div>
    </div>
  </div>
</section>


<!-- =========================================================
     HOW IT WORKS — Buyer-friendly method (data centers version)
========================================================== -->
<section class="page-section" id="how-it-works">
  <h2 class="page-section__title">How it works</h2>
  <p class="page-section__body">
    We don&rsquo;t try to &ldquo;fix everything, all the time.&rdquo; Instead we work in short, practical slices of time &mdash;
    like looking at a security camera frame-by-frame instead of trying to judge a whole day at once.
    In each slice, we line up your sensor streams so you are comparing the same moment, then we re-run the same comparisons
    you already rely on (differences, baselines, alarms) and clearly flag when the results are trustworthy versus when
    the data is too scrambled to call.
  </p>

  <div class="page-section__grid">
    <article class="page-section__card">
      <h3 class="page-section__card-title">1) Pick a calm slice</h3>
      <p class="page-section__card-body">
        We choose short stretches where the system isn&rsquo;t in total chaos (no giant step-changes, no heavy batching).
        Think: a few seconds or minutes where it makes sense to compare sensors fairly.
      </p>
    </article>

    <article class="page-section__card">
      <h3 class="page-section__card-title">2) Measure the time mismatch</h3>
      <p class="page-section__card-body">
        We estimate how far &ldquo;ahead&rdquo; or &ldquo;behind&rdquo; one stream is relative to another &mdash;
        the same way you&rsquo;d align two camera feeds by matching a shared event (fan ramp, door open, load shift).
      </p>
    </article>

    <article class="page-section__card">
      <h3 class="page-section__card-title">3) Line things up</h3>
      <p class="page-section__card-body">
        We shift the data so comparisons are time-fair: sensor A at 12:01:10 is being compared to sensor B at the same moment,
        not &ldquo;roughly around then.&rdquo;
      </p>
    </article>

    <article class="page-section__card">
      <h3 class="page-section__card-title">4) Re-run your normal checks</h3>
      <p class="page-section__card-body">
        Once aligned, differences and baselines behave the way you expected. If a delta remains, it is more likely to be real
        (a genuine hot spot or airflow change) rather than timing noise.
      </p>
    </article>

    <article class="page-section__card">
      <h3 class="page-section__card-title">5) Explain the &ldquo;why&rdquo; when it doesn&rsquo;t work</h3>
      <p class="page-section__card-body">
        If results still look messy, we tell you what kind of problem it is: sensors measuring different things,
        unstable conditions during the slice, or limits in how the data is collected and reported.
      </p>
    </article>

    <article class="page-section__card">
      <h3 class="page-section__card-title">Repeatable evidence</h3>
      <p class="page-section__card-body">
        You get a simple pass/fail style summary for each slice (plus trends over time) &mdash; so a new operator on a new day
        can reach the same conclusion without guessing.
      </p>
    </article>
  </div>
</section>




<!-- =========================================================
     PROOF — What evidence you get (data centers version, plain-English)
========================================================== -->
<section class="page-section" id="proof">
  <h2 class="page-section__title">What proof you&rsquo;ll get</h2>
  <p class="page-section__body">
    We don&rsquo;t ask you to trust a pretty dashboard. We hand you a results package you can show to an operator,
    a facilities lead, or a skeptic in a review meeting &mdash; with clear before/after comparisons, repeatability over time,
    and plain-language reasons when a result can&rsquo;t be trusted.
  </p>

  <div class="page-section__grid">
    <article class="page-section__card">
      <h3 class="page-section__card-title">Before/after you can feel</h3>
      <p class="page-section__card-body">
        We show what your sensor-to-sensor differences looked like before timing alignment and after.
        If a &ldquo;hot spot delta&rdquo; disappears after alignment, it was likely a timing artifact.
        If it remains, it is more likely to be a real physical difference worth chasing.
      </p>
    </article>

    <article class="page-section__card">
      <h3 class="page-section__card-title">Repeatability across time</h3>
      <p class="page-section__card-body">
        We don&rsquo;t rely on one lucky moment. We show whether the same conclusion holds across many short slices:
        stable results when the hall is behaving, and clear flags when conditions (or data collection) make comparisons unreliable.
      </p>
    </article>

    <article class="page-section__card">
      <h3 class="page-section__card-title">Clear reasons when it fails</h3>
      <p class="page-section__card-body">
        When improvement stalls, we tell you which kind of problem you have: sensors truly seeing different things,
        a rapidly changing moment that can&rsquo;t be fairly compared, or limits from polling, buffering, or resampling.
        The goal is fewer &ldquo;shrug&rdquo; tickets and faster root cause.
      </p>
    </article>
  </div>

  <p class="page-section__body" style="margin-top: 1.25rem;">
    <a class="page-section__link" href="PILOTS_URL_HERE">
      See what a pilot deliverable looks like &rarr;
    </a>
  </p>
</section>


        <!-- =========================================================
             EVALUATION — Lead capture / conversion
        ========================================================== -->
        <section class="page-section" id="evaluation">
          <h2 class="page-section__title">Fast evaluation (lead-gen)</h2>
          <p class="page-section__body">
            If you have two or more &ldquo;synchronized-ish&rdquo; channels, we can usually tell quickly whether timing disagreement is
            driving your residuals &mdash; and what the next step should be.
          </p>

          <div class="page-section__grid">
            <article class="page-section__card">
              <h3 class="page-section__card-title">What you provide</h3>
              <p class="page-section__card-body">
                Sample logs (or pilot capture), sampling-rate info, and what should be &ldquo;shared&rdquo; between channels.
              </p>
            </article>

            <article class="page-section__card">
              <h3 class="page-section__card-title">What you get back</h3>
              <p class="page-section__card-body">
                Co-timing results, cancellation improvement metrics, stability diagnostics,
                and a go/no-go recommendation.
              </p>
            </article>

            <article class="page-section__card">
              <h3 class="page-section__card-title">If it&rsquo;s a fit</h3>
              <p class="page-section__card-body">
                We define pilot success criteria (residual reduction plus stability plus reporting format)
                and run a limited field evaluation with repeatable evidence artifacts.
              </p>
            </article>
          </div>

          <p class="page-section__body" style="margin-top: 1.25rem;">
            <a class="page-section__link" href="FORM_URL_HERE">
              Open the 90-second evaluation form &rarr;
            </a>
          </p>
        </section>

        <!-- =========================================================
             NEXT STEP / CONTACT
        ========================================================== -->
        <section class="page-section" id="next-step">
          <h2 class="page-section__title">Next step</h2>
          <p class="page-section__body">
            Want a quick read on whether timing disagreement is polluting your inference?
            Send sample data or request a pilot scope.
          </p>

          <p class="page-section__body">
            <a
              class="page-section__link"
              href="mailto:jonathan.nacionales@hermodlabs.com?subject=HermodLabs%20Data%20Evaluation%20Request"
            >
              jonathan.nacionales@hermodlabs.com
            </a>
          </p>
        </section>
      </main>

      <footer class="site-footer">
        <p class="site-footer__text">
          &copy; <span id="year"></span> HermodLabs. Evidence artifacts and pilots may be limited.
        </p>
      </footer>
    </div>

    <script>
      document.getElementById("year").textContent = new Date().getFullYear();
    </script>
  </body>
</html>

<script type="module" src="/components/site-header.js"></script>
